# -*- coding: utf-8 -*-
"""Afrik_hotels_gpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XaddGFDeVoDq9HeQ0Hm7sSE1hibLVAIK
"""

from google.colab import files
files.upload()

import os, shutil

# Make sure the .kaggle directory exists
os.makedirs("/root/.kaggle", exist_ok=True)

# Move the uploaded file to the correct path
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")

# Set proper permissions
os.chmod("/root/.kaggle/kaggle.json", 600)

# Verify
!ls -la /root/.kaggle
!cat /root/.kaggle/kaggle.json

!kaggle datasets download -d "raj713335/tbo-hotels-dataset"

!unzip tbo-hotels-dataset.zip

import pandas as pd

!pip install chardet
import chardet

# detect encoding by reading a small chunk
rawdata = open('/content/hotels.csv', 'rb').read(200000)
result = chardet.detect(rawdata)
print(result)

df = pd.read_csv('/content/hotels.csv', encoding='Windows-1252', low_memory=False)

df.head(5)

df.isnull().sum()

df.columns.tolist()

df.columns = df.columns.str.strip()

df['countyName'].unique()

df[df['countyName'] == 'Nigeria']

df = df.drop(columns=["countyCode"])

df.shape

african_countries = [
    'Nigeria','Cameroon','Morocco', 'Tanzania','Senegal','Zambia'

]
df_africa = df[df['countyName'].isin(african_countries)].copy()

df_africa.shape

!pip install langchain==0.3.27 langchain-core==0.3.72 langchain-text-splitters==0.3.9 langchain-openai==0.2.12 langchain-community==0.3.26

!pip install -U langgraph

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

from langchain.text_splitter import RecursiveCharacterTextSplitter

from openai import OpenAI
import os
from google.colab import userdata
from langchain_openai import ChatOpenAI

openai_api_key = userdata.get('OPENAI_API_KEY')

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    api_key=openai_api_key
)

from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS

from langchain_core.prompts import ChatPromptTemplate

print("✅ All imports successful!")

from openai import OpenAI
import os
from google.colab import userdata
from langchain_openai import ChatOpenAI

openai_api_key = userdata.get('OPENAI_API_KEY')

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    api_key=openai_api_key
)

def row_to_text(row):
    return f"""
Hotel Name: {row['HotelName']}
City: {row['cityName']}
Country: {row['countyName']}
Rating: {row['HotelRating']}
Address: {row['Address'] if pd.notna(row['Address']) else 'None'}
Phone: {row['PhoneNumber'] if pd.notna(row['PhoneNumber']) else 'None'}
Fax: {row['FaxNumber'] if pd.notna(row['FaxNumber']) else 'None'}
HotelCode: {row['HotelCode']}
Facilities: {row['HotelFacilities'] if pd.notna(row['HotelFacilities']) else 'None'}
Description: {row['Description'] if pd.notna(row['Description']) else 'None'}
Attractions: {row['Attractions'] if pd.notna(row['Attractions']) else 'None'}
Website: {row['HotelWebsiteUrl'] if pd.notna(row['HotelWebsiteUrl']) else 'None'}
Map: {row['Map'] if pd.notna(row['Map']) else 'None'}
"""

from langchain.schema import Document

docs = [Document(page_content=row_to_text(row), metadata=row.to_dict())
        for idx, row in df_africa.iterrows()]

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
all_chunks = []

for doc in docs:
    chunks = splitter.split_text(doc.page_content)
    # keep original metadata for each chunk
    for c in chunks:
        all_chunks.append(Document(page_content=c, metadata=doc.metadata))

!pip install faiss-cpu

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

emb = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=openai_api_key)
vectorstore = FAISS.from_documents(all_chunks, emb)
retriever = vectorstore.as_retriever()

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

rag_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

from langchain.prompts import PromptTemplate

unified_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "You are an expert hotel assistant for hotels in african countries"
        "Nigeria,Morocco,Tanzania,Senegal,Zambia. Your job is to help users find hotels, "
        "provide information about the hotel, make comparisons, and give recommendations based on their needs.\n\n"

        "CAPABILITIES:\n"
        "1. Provide contact details (address, phone, fax, website)\n "
        "2. Recommend hotels based on requirements (facilities, location, rating)\n"
        "3. Compare multiple hotels on features, facilities, and location\n"
        "4. Answer questions about hotel facilities and services\n"
        "5. Suggest hotels near specific attractions or areas\n\n"

        "INSTRUCTIONS:\n"
        "- Use ALL available information from the context\n"
        "- For contact queries: provide address, phone, fax (if available), and website\n"
        "- For recommendations: match user requirements with hotel facilities and rating\n"
        "- For comparisons: highlight key differences in facilities, location, and features\n"
        "- For facility questions: look in the HotelFacilities field\n"
        "- Be specific and include hotel names, ratings, and relevant details\n"
        "- If information is not available, say so clearly\n\n"

        "Context:\n{context}\n\n"
        "User Question: {question}\n\n"
        "Answer:"
    )
)


rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    combine_docs_chain_kwargs={"prompt": unified_prompt}
)

query = "what five star hotel in lagos do you recommend for someone with 5 kids?"
result = rag_chain.invoke({"question": query})
print(result["answer"])

!pip install google-search-results

from google.colab import userdata
from serpapi import GoogleSearch
import os

API_KEY = userdata.get("SERPAPI_API_KEY")

def search_phone_number(query):
    params = {
        "engine": "google",
        "q": query,
        "api_key": API_KEY,
        "num": "10"
    }
    search = GoogleSearch(params)
    results = search.get_dict()

    if "knowledge_graph" in results:
        kg = results["knowledge_graph"]
        if "phone" in kg:
            return kg["phone"]
        if "customer_service" in kg:
            return kg["customer_service"]

    if "local_results" in results:
        for result in results["local_results"]:
            if "phone" in result:
                return result["phone"]

    if "organic_results" in results:
        for result in results["organic_results"][:5]:
            snippet = result.get("snippet", "")
            if any(keyword in snippet.lower() for keyword in ["phone", "tel", "call", "+234"]):
                return snippet

    return "Not found"

query = "rockview hotel Lagos phone number"
phone_number = search_phone_number(query)
print(phone_number)

from langgraph.graph import StateGraph, END
from typing import TypedDict

class HotelAgentState(TypedDict):
    query: str
    rag_answer: str
    final_answer: str

def rag_node(state: HotelAgentState):
    """Get answer from hotel dataset using RAG chain"""
    query = state["query"]
    result = rag_chain.invoke({"question": query})
    answer = result["answer"]

    return {
        "query": query,
        "rag_answer": answer,
        "final_answer": answer
    }

def should_search_online(state: HotelAgentState) -> str:
    """Decide if there is need to search for phone number online"""
    query = state["query"].lower()
    answer = state["rag_answer"].lower()

    asking_for_contact = any(word in query for word in
                            ["phone", "contact", "call", "number", "reach", "telephone"])


    has_phone = ("+234" in state["rag_answer"] or
                 "phone:" in answer or
                 "tel:" in answer)


    phone_unavailable = any(phrase in answer for phrase in
                           ["not available", "no phone", "not found", "nan"])

    # Search online only if user wants phone AND (phone is missing OR unavailable)
    if asking_for_contact and (not has_phone or phone_unavailable):
        return "search_online"
    else:
        return "finish"

def search_online_node(state: HotelAgentState):
    """Search for phone number online using SerpAPI"""
    query = state["query"]

    search_query = f"{query} Nigeria contact phone"
    phone_result = search_phone_number(search_query)

    # Combine RAG answer with online search result
    if phone_result != "Not found":
        enhanced_answer = (
            f"{state['rag_answer']}\n\n"
            f" Additional Contact Information (from online search):\n"
            f"{phone_result}"
        )
    else:
        enhanced_answer = (
            f"{state['rag_answer']}\n\n"
            f" Unable to find additional contact information online."
        )

    return {
        "query": state["query"],
        "rag_answer": state["rag_answer"],
        "final_answer": enhanced_answer
    }

def finish_node(state: HotelAgentState):
    """No changes needed, just return final answer"""
    return state

# Step 6: Build the LangGraph Workflow
workflow = StateGraph(HotelAgentState)

# Add all nodes
workflow.add_node("rag", rag_node)
workflow.add_node("search_online", search_online_node)
workflow.add_node("finish", finish_node)

workflow.set_entry_point("rag")

workflow.add_conditional_edges(
    "rag",
    should_search_online,
    {
        "search_online": "search_online",
        "finish": "finish"
    }
)
workflow.add_edge("search_online", END)
workflow.add_edge("finish", END)

hotel_agent = workflow.compile()

print("✅ Hotel Agent with LangGraph created successfully!")

# Visualize the graph
from IPython.display import Image, display
display(Image(hotel_agent.get_graph().draw_mermaid_png()))

def ask_hotel_agent(question: str) -> str:
    """
    Ask the hotel agent a question

    Args:
        question: User's question about hotels

    Returns:
        Final answer from the agent
    """
    result = hotel_agent.invoke({
        "query": question,
        "rag_answer": "",
        "final_answer": ""
    })

    return result["final_answer"]

import gradio as gr

def chat_with_agent(message, history):

    try:
        response = ask_hotel_agent(message)
        return response
    except Exception as e:
        return f"Sorry, I encountered an error: {str(e)}"


demo = gr.ChatInterface(
    fn=chat_with_agent,
    title=" Africa Hotel Assistant",
    description="Ask me anything about hotels in Africa! I can help with recommendations, contact information, facilities,rating and more.",
    theme=gr.themes.Soft()
)


demo.launch(share=True)

